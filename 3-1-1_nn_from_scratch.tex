\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{listings}
\usepackage{xcolor}

\title{Neural Network from Scratch}
\author{Your Name}
\date{}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=5pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=2
}

\lstset{style=mystyle}

\begin{document}

\maketitle

\section{Introduction}

In this document I am going to go over my implementation of a very simple Neural Network and the math behind it.

\section{Network Architecture}

My simple neural network consists of:

\begin{enumerate}
    \item \textbf{Input Layer}: 3-dimensional input
    \item \textbf{Hidden Layer}: One neuron
    \item \textbf{Output Layer}: 1-dimensional output
\end{enumerate}

\section{Initialization}

We generate/initialize the weights using the Xavier Initialization:

\begin{equation}
    W \sim N(0, \frac{2}{n_{in} + n_{out}})
\end{equation}

Where:
\begin{itemize}
    \item $n_{in}$ is the number of input features
    \item $n_{out}$ is the number of neurons in the layer
    \item $N$ is the normal distribution function
\end{itemize}

In our Python implementation:

\begin{lstlisting}[language=Python]
n_in = 3
n_out = 1
w = np.random.normal(0, np.sqrt(2 / (n_in + n_out)), size=(n_in, n_out))
b = 0
\end{lstlisting}

\section{Forward Propagation}

In the Forward Propagation we pass the input data through the Neural Network to generate an output.

\begin{enumerate}
    \item \textbf{Linear Combination}: Calculate the weighted sum of inputs plus a bias:

    \begin{equation}
        Z = W \cdot X + b
    \end{equation}

    \item \textbf{Activation Function}: Apply a non-linear function, in this case Sigmoid, to introduce some non-linearity:

    \begin{equation}
        A = \frac{1}{1 + e^{-Z}}
    \end{equation}
\end{enumerate}

In our code:

\begin{lstlisting}[language=Python]
z = np.dot(w.T, x) + b
a = sigmoid(z)
\end{lstlisting}

Where \texttt{sigmoid} is defined as:

\begin{lstlisting}[language=Python]
def sigmoid(z):
    return 1 / (1 + np.exp(-z))
\end{lstlisting}

\section{Loss Function}

We use the Mean Squared Error (MSE) to calculate the loss of this epoch and evaluate how the model is doing:

\begin{equation}
    L = \frac{1}{2}(A - Y)^2
\end{equation}

Where:
\begin{itemize}
    \item $A$ is the predicted output
    \item $Y$ is the actual target value
\end{itemize}

\section{Backpropagation}

In the Backpropagation process we calculate the gradients of the loss function with respect to the weights and the biases to then be able to update them accordingly.

\subsection{Derivatives}

We compute the following partial derivatives:

\subsubsection{Derivative of Loss with respect to Activation}

We start with the Mean Squared Error loss function:

\begin{equation}
    L = \frac{1}{2}(A - Y)^2
\end{equation}

To find $\frac{\partial L}{\partial A}$, we apply the chain rule:

\begin{align}
    \frac{\partial L}{\partial A} &= \frac{\partial}{\partial A} [\frac{1}{2}(A - Y)^2] \\
    &= \frac{1}{2} \cdot 2(A - Y) \cdot \frac{\partial}{\partial A}(A - Y) \\
    &= (A - Y) \cdot 1 \\
    &= A - Y
\end{align}

Thus we derive: $\frac{\partial L}{\partial A} = (A - Y)$

\subsubsection{Derivative of Activation with respect to Linear Combination}

We use the sigmoid function as our activation function:

\begin{equation}
    A = \frac{1}{1 + e^{-Z}} = \sigma(Z)
\end{equation}

To find $\frac{\partial A}{\partial Z}$, we use the derivative of the sigmoid function:

\begin{align}
    \frac{\partial A}{\partial Z} &= \frac{\partial}{\partial Z} \sigma(Z) \\
    &= \sigma(Z)(1 - \sigma(Z)) \\
    &= A(1 - A)
\end{align}

$A = \sigma(Z)$ Thus, we derive: $\frac{\partial A}{\partial Z} = A(1 - A)$

\subsubsection{Derivative of Linear Combination with respect to Weights}

Our linear combination is:

\begin{equation}
    Z = W \cdot X + b
\end{equation}

To find $\frac{\partial Z}{\partial W}$, we differentiate with respect to W:

\begin{align}
    \frac{\partial Z}{\partial W} &= \frac{\partial}{\partial W} (W \cdot X + b) \\
    &= \frac{\partial}{\partial W} (W \cdot X) + \frac{\partial}{\partial W} b \\
    &= X + 0 \\
    &= X
\end{align}

Thus, we derive: $\frac{\partial Z}{\partial W} = X$

\subsubsection{Derivative of Linear Combination with respect to Bias}

\begin{equation}
    Z = W \cdot X + b
\end{equation}

To find $\frac{\partial Z}{\partial b}$, we differentiate with respect to b:

\begin{align}
    \frac{\partial Z}{\partial b} &= \frac{\partial}{\partial b} (W \cdot X + b) \\
    &= \frac{\partial}{\partial b} (W \cdot X) + \frac{\partial}{\partial b} b \\
    &= 0 + 1 \\
    &= 1
\end{align}

Thus, we derive: $\frac{\partial Z}{\partial b} = 1$

\subsection{Chain Rule}

We use the chain rule to calculate the gradients of the loss with respect to the weights and bias:

\begin{equation}
    \frac{\partial L}{\partial W} = \frac{\partial L}{\partial A} \cdot \frac{\partial A}{\partial Z} \cdot \frac{\partial Z}{\partial W} = (A - Y) \cdot A(1 - A) \cdot X
\end{equation}

\begin{equation}
    \frac{\partial L}{\partial b} = \frac{\partial L}{\partial A} \cdot \frac{\partial A}{\partial Z} \cdot \frac{\partial Z}{\partial b} = (A - Y) \cdot A(1 - A)
\end{equation}

In our code:

\begin{lstlisting}[language=Python]
dl_dw = (a - y) * a * (1 - a) * x
dl_db = (a - y) * a * (1 - a)
\end{lstlisting}

\section{Gradient Descent}

In the last step we apply an algorithm called Gradient Descent to find the direction for which the weights and biases will be minimal. Afterwards we update them to minimize the loss.

\begin{align}
    W &= W - \eta \cdot \frac{\partial L}{\partial W} \\
    b &= b - \eta \cdot \frac{\partial L}{\partial b}
\end{align}

Where $\eta$ is the learning rate, which controls the step size of each update.

In our implementation:

\begin{lstlisting}[language=Python]
learning_rate = 0.005
w = w - learning_rate * dl_dw
b = b - learning_rate * dl_db
\end{lstlisting}

\section{Training Loop}

In the full code this process gets repeated multiple times to hopefully minimize the loss and get as accurate as possible. In the full code it looks like this:

\begin{lstlisting}[language=Python]
def feedforward_nn(epochs, x, y):
    # ... (initialization code)

    for epoch in range(epochs):
        # Forward propagation
        z = np.dot(w.T, x) + b
        a = sigmoid(z)

        # Backpropagation
        dl_dw = (a - y) * a * (1 - a) * x
        dl_db = (a - y) * a * (1 - a)

        # Gradient descent
        w = w - learning_rate * dl_dw
        b = b - learning_rate * dl_db

        # Print progress (optional)
        if epoch % 100 == 0:
            print(f"Epoch {epoch}: w = {list(map('{:.3f}%'.format, w.flatten()))}, b = {list(map('{:.3f}%'.format,b))}, loss = {list(map('{:.3f}%'.format,(a - y) ** 2))}")

    return w, b
\end{lstlisting}
Hope you enjoyed my implementation:)
\end{document}